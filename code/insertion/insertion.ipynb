{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"neo4j://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "dbname = \"neo4j\"\n",
    "jar_path = \"../../utils/jars/neo4j-connector-apache-spark_2.12-5.3.8_for_spark_3.jar\"\n",
    "me = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    concat_ws,\n",
    "    to_date,\n",
    "    initcap,\n",
    "    trim,\n",
    "    regexp_replace,\n",
    "    monotonically_increasing_id,\n",
    "    lit,\n",
    "    col,\n",
    "    udf,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    ")\n",
    "from neo4j import GraphDatabase\n",
    "import unicodedata, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    return (\n",
    "        SparkSession.builder.appName(\"Neo4jIntegration\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            jar_path,\n",
    "        )\n",
    "        .config(\"spark.neo4j.url\", \"bolt://localhost:7687\")\n",
    "        .config(\"spark.neo4j.authentication.type\", \"basic\")\n",
    "        .config(\"spark.neo4j.authentication.basic.username\", username)\n",
    "        .config(\"spark.neo4j.authentication.basic.password\", password)\n",
    "        .config(\"neo4j.url\", url)\n",
    "        .config(\"neo4j.authentication.type\", \"basic\")\n",
    "        .config(\"neo4j.authentication.basic.username\", username)\n",
    "        .config(\"neo4j.authentication.basic.password\", password)\n",
    "        .config(\"neo4j.database\", dbname)\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print(\"ðŸ”„ No existing SparkSession found. Creating a new one...\")\n",
    "    spark = create_spark_session()\n",
    "    print(\"âœ… SparkSession created successfully.\")\n",
    "else:\n",
    "    if spark._jsparkSession is None:\n",
    "        print(\"âš ï¸ Existing SparkSession is not active. Recreating it...\")\n",
    "        spark = create_spark_session()\n",
    "        print(\"âœ… SparkSession re-created successfully.\")\n",
    "    else:\n",
    "        print(\"âœ… SparkSession is already active.\")\n",
    "\n",
    "print(f\"ðŸ”¥ Spark is running â€” version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"List of jars:\")\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nodes(df, label, key_cols, mode=\"Overwrite\"):\n",
    "    (\n",
    "        df.write.format(\"org.neo4j.spark.DataSource\")\n",
    "        .mode(mode)\n",
    "        .option(\"labels\", f\":{label}\")\n",
    "        .option(\"node.keys\", \",\".join(key_cols))\n",
    "        .save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rels(\n",
    "    df,\n",
    "    rel_type,\n",
    "    src_label,\n",
    "    src_key,\n",
    "    src_col,\n",
    "    tgt_label,\n",
    "    tgt_key,\n",
    "    tgt_col,\n",
    "    prop_cols=None,\n",
    "    mode=\"Overwrite\",\n",
    "):\n",
    "    w = (\n",
    "        df.write.format(\"org.neo4j.spark.DataSource\")\n",
    "        .mode(mode)\n",
    "        .option(\"relationship\", rel_type)\n",
    "        .option(\"relationship.save.strategy\", \"keys\")\n",
    "        .option(\"relationship.source.labels\", f\":{src_label}\")\n",
    "        .option(\"relationship.source.node.keys\", f\"{src_col}:{src_key}\")\n",
    "        .option(\"relationship.target.labels\", f\":{tgt_label}\")\n",
    "        .option(\"relationship.target.node.keys\", f\"{tgt_col}:{tgt_key}\")\n",
    "    )\n",
    "    if prop_cols:\n",
    "        w = w.option(\"relationship.properties\", \",\".join(prop_cols))\n",
    "    w.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(colname: str):\n",
    "    no_web = regexp_replace(col(colname), r\"\\.(com|net|org|io)$\", \"\")\n",
    "    return initcap(\n",
    "        trim(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    regexp_replace(no_web, r\"\\(.*?\\)\", \" \"),\n",
    "                    r\",.*$\",\n",
    "                    \"\",\n",
    "                ),\n",
    "                r\"\\s+\",\n",
    "                \" \",\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, trim, initcap\n",
    "\n",
    "\n",
    "def clean_name(colname: str):\n",
    "    no_titles = regexp_replace(col(colname), r\",\\s*(M?Sc\\.?|B?Sc\\.?|PhD\\.?|MBA\\.?)\", \"\")\n",
    "    no_parens = regexp_replace(no_titles, r\"\\(.*?\\)\", \" \")\n",
    "    no_loose_dots = regexp_replace(no_parens, r\"\\s*\\.(?=\\S)\", \"\")\n",
    "    single_space = trim(regexp_replace(no_loose_dots, r\"\\s+\", \" \"))\n",
    "    return initcap(single_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def clear_db():\n",
    "\n",
    "driver = GraphDatabase.driver(url, auth=(username, password))\n",
    "with driver.session() as session:\n",
    "    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "driver.close()\n",
    "\"\"\"\n",
    "\n",
    "# clear_db()\n",
    "# print(\"Database cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Positions.csv\")\n",
    "    .withColumnRenamed(\"Company Name\", \"company\")\n",
    "    .withColumn(\"from\", F.to_date(F.col(\"Started On\"), \"MMM yyyy\").cast(DateType()))\n",
    "    .withColumn(\"to\", F.to_date(F.col(\"Finished On\"), \"MMM yyyy\").cast(DateType()))\n",
    "    .withColumn(\"user\", F.lit(me))\n",
    "    .select(\n",
    "        \"user\",\n",
    "        \"company\",\n",
    "        F.col(\"Title\").alias(\"title\"),\n",
    "        F.col(\"Location\").alias(\"location\"),\n",
    "        \"from\",\n",
    "        \"to\",\n",
    "    )\n",
    "    .dropDuplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Education.csv\")\n",
    "    .withColumnRenamed(\"School Name\", \"school\")\n",
    "    .withColumn(\"from\", F.to_date(F.col(\"Start Date\"), \"MMM yyyy\").cast(DateType()))\n",
    "    .withColumn(\"to\", F.to_date(F.col(\"End Date\"), \"MMM yyyy\").cast(DateType()))\n",
    "    .withColumn(\"user\", F.lit(me))\n",
    "    .select(\"user\", \"school\", F.col(\"Degree Name\").alias(\"degree\"), \"from\", \"to\")\n",
    "    .dropDuplicates()\n",
    ")\n",
    "universities = (\n",
    "    education_df.select(\"school\").distinct().withColumnRenamed(\"school\", \"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Languages.csv\")\n",
    "    .withColumnRenamed(\"Name\", \"name\")\n",
    "    .withColumnRenamed(\"Proficiency\", \"proficiency\")\n",
    "    .withColumn(\"name\", F.initcap(\"name\"))\n",
    "    .select(\"name\", \"proficiency\")\n",
    "    .dropna(subset=[\"name\"])\n",
    "    .dropDuplicates([\"name\", \"proficiency\"])\n",
    ")\n",
    "\n",
    "\n",
    "user_langs_df = langs_df.withColumn(\"user\", F.lit(me)).select(\n",
    "    F.col(\"user\"), F.col(\"name\"), F.col(\"proficiency\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_certs_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Certifications.csv\")\n",
    "    .withColumnRenamed(\"Name\", \"name\")\n",
    "    .withColumnRenamed(\"Authority\", \"authority\")\n",
    "    .withColumn(\"name\", F.trim(F.col(\"name\")))\n",
    "    .withColumn(\"authority\", F.trim(F.col(\"authority\")))\n",
    "    .withColumn(\"start\", F.to_date(F.col(\"Started On\"), \"MMM yyyy\").cast(DateType()))\n",
    "    .withColumn(\"license\", F.col(\"License Number\"))\n",
    "    .dropDuplicates([\"name\", \"authority\"])\n",
    ")\n",
    "\n",
    "certs_df = raw_certs_df.select(\"name\", \"authority\").dropDuplicates(\n",
    "    [\"name\", \"authority\"]\n",
    ")\n",
    "user_certs_df = (\n",
    "    raw_certs_df.withColumn(\"user\", F.lit(me))\n",
    "    .select(\"user\", \"name\", \"authority\", \"start\", \"license\")\n",
    "    .dropDuplicates([\"user\", \"name\", \"authority\", \"start\", \"license\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_jobs_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Saved Jobs.csv\")\n",
    "    .withColumnRenamed(\"Job Title\", \"title\")\n",
    "    .withColumnRenamed(\"Company Name\", \"company\")\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"savedDate\", F.to_timestamp(F.col(\"Saved Date\"), \"M/d/yy, h:mm a\"))\n",
    "    .withColumn(\"title\", F.trim(F.col(\"title\")))\n",
    "    .withColumn(\"company\", F.trim(F.col(\"company\")))\n",
    ")\n",
    "\n",
    "jobs_df = raw_jobs_df.select(\"id\", \"title\", \"company\").dropDuplicates([\"id\"])\n",
    "\n",
    "user_saved_df = raw_jobs_df.select(\n",
    "    F.lit(me).alias(\"user\"), \"id\", \"savedDate\"\n",
    ").dropDuplicates([\"user\", \"id\", \"savedDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Skills.csv\")\n",
    "    .withColumn(\"name\", F.initcap(F.col(\"Name\")))\n",
    "    .withColumn(\"nameLower\", F.lower(F.col(\"name\")))\n",
    "    .select(\"name\", \"nameLower\")\n",
    "    .dropna(subset=[\"name\"])\n",
    "    .dropDuplicates([\"nameLower\"])\n",
    ")\n",
    "\n",
    "user_skills_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Skills.csv\")\n",
    "    .withColumn(\"user\", F.lit(me))\n",
    "    .withColumn(\"skill\", F.initcap(F.col(\"Name\")))\n",
    "    .select(F.col(\"user\"), F.col(\"skill\"))\n",
    "    .dropDuplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "follows_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Company Follows.csv\")\n",
    "    .withColumnRenamed(\"Organization\", \"company\")\n",
    "    .withColumn(\n",
    "        \"since\", F.to_timestamp(F.col(\"Followed On\"), \"EEE MMM dd HH:mm:ss z yyyy\")\n",
    "    )\n",
    "    .withColumn(\"user\", F.lit(me))\n",
    "    .withColumn(\"company\", clean(\"company\"))\n",
    "    .select(\"user\", \"company\", \"since\")\n",
    "    .dropDuplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.read.option(\"header\", True).csv(base_path + \"Connections.csv\")\n",
    "\n",
    "connections_work_df = (\n",
    "    raw.withColumn(\"raw_name\", concat_ws(\" \", col(\"First Name\"), col(\"Last Name\")))\n",
    "    .withColumn(\"user\", clean(\"raw_name\"))\n",
    "    .withColumn(\"company\", clean(\"Company\"))\n",
    "    .withColumn(\"title\", trim(col(\"Position\")))\n",
    "    .select(\"user\", \"company\", \"title\")\n",
    "    .dropna(subset=[\"user\", \"company\"])\n",
    "    .dropDuplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Invitations.csv\")\n",
    "    .withColumnRenamed(\"From\", \"fromName\")\n",
    "    .withColumnRenamed(\"To\", \"toName\")\n",
    "    .withColumn(\"date\", F.to_timestamp(F.col(\"Sent At\"), \"M/d/yy, h:mm a\"))\n",
    "    .withColumnRenamed(\"Direction\", \"direction\")\n",
    "    .select(\"fromName\", \"toName\", \"date\", \"direction\")\n",
    "    .dropDuplicates()\n",
    ")\n",
    "invitation_users_df = (\n",
    "    inv_df.select(F.col(\"fromName\").alias(\"name\"))\n",
    "    .union(inv_df.select(F.col(\"toName\").alias(\"name\")))\n",
    "    .distinct()\n",
    "    .sort(\"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = (\n",
    "    positions_df.select(\"company\")\n",
    "    .union(jobs_df.select(\"company\"))\n",
    "    .union(follows_df.select(\"company\"))\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"company\", \"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.read.option(\"header\", True).csv(base_path + \"Connections.csv\")\n",
    "\n",
    "connections_users_df = (\n",
    "    raw.withColumn(\n",
    "        \"raw_name\", concat_ws(\" \", trim(col(\"First Name\")), trim(col(\"Last Name\")))\n",
    "    )\n",
    "    .withColumn(\"name\", clean_name(\"raw_name\"))\n",
    "    .select(\"name\")\n",
    "    .dropna()\n",
    "    .dropDuplicates()\n",
    "    .filter(col(\"name\") != \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_from_positions = positions_df.select(F.col(\"user\").alias(\"name\")).distinct()\n",
    "all_users_df = (\n",
    "    users_from_positions.union(connections_users_df)\n",
    "    .union(invitation_users_df)\n",
    "    .withColumn(\"name\", clean(\"name\"))\n",
    "    .distinct()\n",
    "    .withColumn(\"name\", initcap(trim(regexp_replace(col(\"name\"), \",.*$-\", \"\"))))\n",
    "    .filter(col(\"name\").isNotNull() & (col(\"name\") != \"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.read.option(\"header\", True).csv(base_path + \"Connections.csv\")\n",
    "\n",
    "conns_df = (\n",
    "    raw.withColumn(\"nameA\", concat_ws(\" \", col(\"First Name\"), col(\"Last Name\")))\n",
    "    .withColumn(\"nameA\", clean_name(\"nameA\"))\n",
    "    .withColumn(\"nameB\", F.lit(me))\n",
    "    .withColumn(\"since\", F.to_date(trim(col(\"Connected On\")), \"dd MMM yyyy\"))\n",
    "    .select(\"nameA\", \"nameB\", \"since\")\n",
    "    .dropna(subset=[\"nameA\"])\n",
    "    .dropDuplicates([\"nameA\", \"nameB\", \"since\"])\n",
    "    .filter(col(\"nameA\").isNotNull() & (col(\"nameA\") != \"\"))\n",
    "    .filter(col(\"nameB\").isNotNull() & (col(\"nameB\") != \"\"))\n",
    "    .filter(col(\"nameA\") != \"\")\n",
    "    .filter(col(\"since\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df = (\n",
    "    positions_df.select(F.col(\"company\").alias(\"name\"))\n",
    "    .union(connections_work_df.select(F.col(\"company\").alias(\"name\")))\n",
    "    .union(follows_df.select(F.col(\"company\").alias(\"name\")))\n",
    "    .union(jobs_df.select(F.col(\"Company\").alias(\"name\")))\n",
    "    .withColumn(\"name\", clean(\"name\"))\n",
    "    .distinct()\n",
    "    .filter(col(\"name\") != \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Education.csv\")\n",
    "    .withColumnRenamed(\"School Name\", \"school\")\n",
    "    .withColumn(\"from\", F.to_date(F.col(\"Start Date\"), \"MMM yyyy\").cast(DateType()))\n",
    "    .withColumn(\"to\", F.to_date(F.col(\"End Date\"), \"MMM yyyy\").cast(DateType()))\n",
    "    .withColumn(\"user\", F.lit(me))\n",
    "    .select(\"user\", \"school\", F.col(\"Degree Name\").alias(\"degree\"), \"from\", \"to\")\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "universities = (\n",
    "    education_df.select(\"school\").distinct().withColumnRenamed(\"school\", \"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes(all_users_df, \"User\", [\"name\"])\n",
    "print(f\"Writting {all_users_df.count()} USER nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes(companies_df, \"Company\", [\"name\"])\n",
    "print(f\"Writting {companies_df.count()} COMPANY nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes(universities, \"University\", [\"name\"])\n",
    "print(f\"Writting {universities.count()} UNIVERSITY nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes(langs_df.select(\"name\"), \"Language\", [\"name\"])\n",
    "print(f\"Writting {langs_df.count()} LANGUAGE nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes(\n",
    "    raw_certs_df.select(\"name\", \"authority\"), \"Certification\", [\"name\", \"authority\"]\n",
    ")\n",
    "print(f\"Writting {raw_certs_df.count()} CERTIFICATION nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nodes(skills_df.select(\"name\"), \"Skill\", [\"name\"])\n",
    "print(f\"Writting {skills_df.count()} SKILL nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_nodes_df = jobs_df.select(\"id\", \"title\", \"company\").dropDuplicates([\"id\"])\n",
    "write_nodes(jobs_nodes_df, \"Job\", [\"id\"])\n",
    "print(f\"Writting {jobs_nodes_df.count()} JOB nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    education_df.withColumn(\"user\", lit(me)).select(\n",
    "        \"user\", \"school\", \"degree\", \"from\", \"to\"\n",
    "    ),\n",
    "    \"STUDIED_AT\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"University\",\n",
    "    \"name\",\n",
    "    \"school\",\n",
    "    prop_cols=[\"degree\", \"from\", \"to\"],\n",
    ")\n",
    "print(f\"Writting {education_df.count()} STUDIED_AT relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    langs_df.withColumn(\"user\", lit(me)).select(\"user\", \"name\", \"proficiency\"),\n",
    "    \"SPEAKS\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"Language\",\n",
    "    \"name\",\n",
    "    \"name\",\n",
    "    prop_cols=[\"proficiency\"],\n",
    ")\n",
    "print(f\"Writting {langs_df.count()} SPEAKS relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    raw_certs_df.withColumn(\"user\", lit(me)).select(\n",
    "        \"user\", \"name\", \"authority\", \"start\", \"license\"\n",
    "    ),\n",
    "    \"HAS_CERT\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"Certification\",\n",
    "    \"name\",\n",
    "    \"name\",\n",
    "    prop_cols=[\"start\", \"license\"],\n",
    ")\n",
    "print(f\"Writting {raw_certs_df.count()} HAS_CERT relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    skills_df.withColumn(\"user\", lit(me)).select(\"user\", \"name\"),\n",
    "    \"HAS_SKILL\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"Skill\",\n",
    "    \"name\",\n",
    "    \"name\",\n",
    ")\n",
    "print(f\"Writting {skills_df.count()} HAS_SKILL relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    positions_df.withColumn(\"user\", lit(me)).select(\n",
    "        \"user\", \"company\", \"title\", \"location\", \"from\", \"to\"\n",
    "    ),\n",
    "    \"WORKED_AT\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"Company\",\n",
    "    \"name\",\n",
    "    \"company\",\n",
    "    prop_cols=[\"title\", \"location\", \"from\", \"to\"],\n",
    ")\n",
    "print(f\"Writting {positions_df.count()} WORKED_AT relationships\")\n",
    "write_rels(\n",
    "    connections_work_df,\n",
    "    \"WORKED_AT\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"Company\",\n",
    "    \"name\",\n",
    "    \"company\",\n",
    "    prop_cols=[\"title\"],\n",
    ")\n",
    "print(f\"Writting {connections_work_df.count()} WORKED_AT relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    inv_df.select(\"fromName\", \"toName\", \"date\", \"direction\"),\n",
    "    \"INVITED\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"fromName\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"toName\",\n",
    "    prop_cols=[\"date\", \"direction\"],\n",
    ")\n",
    "print(f\"Writting {inv_df.count()} INVITED relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    conns_df,\n",
    "    \"CONNECTED\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"nameA\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"nameB\",\n",
    "    prop_cols=[\"since\"],\n",
    ")\n",
    "print(f\"Writting {conns_df.count()} CONNECTED relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    follows_df,\n",
    "    \"FOLLOWS\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"Company\",\n",
    "    \"name\",\n",
    "    \"company\",\n",
    "    prop_cols=[\"since\"],\n",
    ")\n",
    "print(f\"Writting {follows_df.count()} FOLLOWS relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rels(\n",
    "    user_saved_df,\n",
    "    \"SAVED_JOB\",\n",
    "    \"User\",\n",
    "    \"name\",\n",
    "    \"user\",\n",
    "    \"Job\",\n",
    "    \"id\",\n",
    "    \"id\",\n",
    "    prop_cols=[\"savedDate\"],\n",
    ")\n",
    "print(f\"Writing {user_saved_df.count()} SAVED_JOB relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_connections = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"relationship\", \"CONNECTED\")\n",
    "    .option(\"relationship.source.labels\", \":User\")\n",
    "    .option(\"relationship.target.labels\", \":User\")\n",
    "    .option(\"relationship.source.node.keys\", \"name:name\")\n",
    "    .option(\"relationship.target.node.keys\", \"name:name\")\n",
    "    .load()\n",
    "    .selectExpr(\n",
    "        \"`source.name` AS nameA\", \"`target.name` AS nameB\", \"`rel.since` AS since\"\n",
    "    )\n",
    ")\n",
    "\n",
    "csv_connections = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Connections.csv\")\n",
    "    .withColumn(\"nameA\", concat_ws(\" \", col(\"First Name\"), col(\"Last Name\")))\n",
    "    .withColumn(\"nameB\", lit(me))\n",
    "    .withColumnRenamed(\"Connected On\", \"since\")\n",
    "    .withColumn(\"since\", to_date(\"since\", \"dd MMM yyyy\"))\n",
    "    .select(\"nameA\", \"nameB\", \"since\")\n",
    "    .dropDuplicates([\"nameA\", \"nameB\", \"since\"])\n",
    ")\n",
    "\n",
    "missing_connections = csv_connections.join(\n",
    "    neo_connections, on=[\"nameA\", \"nameB\", \"since\"], how=\"left_anti\"\n",
    ")\n",
    "print(f\"Missing connections in Neo4j: {missing_connections.count()}\")\n",
    "missing_connections.show(500, truncate=False)\n",
    "\n",
    "csv_users = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(base_path + \"Connections.csv\")\n",
    "    .withColumn(\"name\", concat_ws(\" \", col(\"First Name\"), col(\"Last Name\")))\n",
    "    .select(\"name\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "neo_users = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"labels\", \":User\")\n",
    "    .option(\"node.keys\", \"name\")\n",
    "    .load()\n",
    "    .select(\"name\")\n",
    ")\n",
    "\n",
    "missing_users = csv_users.join(neo_users, on=\"name\", how=\"left_anti\")\n",
    "print(f\"Missing users in Neo4j: {missing_users.count()}\")\n",
    "missing_users.show(truncate=False)\n",
    "\n",
    "neo_companies = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"labels\", \":Company\")\n",
    "    .option(\"node.keys\", \"name\")\n",
    "    .load()\n",
    "    .select(\"name\")\n",
    ")\n",
    "\n",
    "missing_companies = companies_df.join(neo_companies, on=\"name\", how=\"left_anti\")\n",
    "print(f\"Missing companies in Neo4j: {missing_companies.count()}\")\n",
    "missing_companies.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates():\n",
    "    driver = GraphDatabase.driver(url, auth=(username, password))\n",
    "\n",
    "    queries = [\n",
    "        \"\"\"\n",
    "        MATCH (n:User)\n",
    "        WITH n.name AS name, COUNT(*) AS count\n",
    "        WHERE count > 1\n",
    "        RETURN name, count\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        MATCH (n:Company)\n",
    "        WITH n.name AS name, COUNT(*) AS count\n",
    "        WHERE count > 1\n",
    "        RETURN name, count\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        MATCH (n:University)\n",
    "        WITH n.name AS name, COUNT(*) AS count\n",
    "        WHERE count > 1\n",
    "        RETURN name, count\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        MATCH (n:Skill)\n",
    "        WITH n.name AS name, COUNT(*) AS count\n",
    "        WHERE count > 1\n",
    "        RETURN name, count\n",
    "        \"\"\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            for query in queries:\n",
    "                result = session.run(query)\n",
    "                print(\"\\nResults:\")\n",
    "                for record in result:\n",
    "                    print(f\"Nombre: {record['name']}, Conteo: {record['count']}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "check_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark351",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
